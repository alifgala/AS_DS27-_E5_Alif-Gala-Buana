# -*- coding: utf-8 -*-
"""AS_DS27+_E5_NamaLengkap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gELKoWxtaWhCk8Y7In697QDdf6-xV0he

# Install & Load Package
"""

!pip install scipy==1.11.4
!pip install dalex
!pip install scikit-plot
!pip install shap
!pip install lime

# import pandas for data wrangling
import pandas as pd
# import numpy for vectorize data manipulation
import numpy as np
# import matplotlib.pyplot module for data visualization
import matplotlib.pyplot as plt
# import seaborn for data visualization
import seaborn as sns
# import scipy for certain statistical function
from scipy import stats

# import train and test split method from scikit-learn
from sklearn.model_selection import train_test_split
# import metrics method for model evaluation
import sklearn.metrics as metrics
# import random forest classifier
from sklearn.ensemble import RandomForestClassifier
# import multi-layer perceptron
from sklearn.neural_network import MLPClassifier
# import decision tree model as surrogate model
from sklearn.tree import DecisionTreeClassifier
# import tree module
from sklearn import tree

# import xgboost classifier
from xgboost import XGBClassifier

# import dalex to explain complex model
import dalex as dx

# load scikit-plot modules
import scikitplot as skplt

# load shap package for shap explanation
import shap

# load LimeTabularExplainer for LIME method
from lime.lime_tabular import LimeTabularExplainer

"""# Load Dataset"""

# load customer churn dataset
churn_data = pd.read_csv("https://raw.githubusercontent.com/hadimaster65555/dataset_for_teaching/main/dataset/bank_churn_dataset/bank_churn_data.csv")

# check the first 5 data frim churn_data
churn_data.head()

"""# Data Inspection"""

# check churn_data data structure and its types
churn_data.dtypes

# check data dimension
churn_data.shape

# check null values inside dataset
churn_data.isna().sum()

# removes row with na values
churn_data = churn_data.dropna()

# check dimension and null values inside dataset
print("Dimension of data: {}".format(churn_data.shape))
print("Number of null values:")
churn_data.isna().sum()

# drop custid column
churn_data = churn_data.drop(['user_id'], axis = 1)

"""# Simple EDA"""

# check target class distribution
churn_data['attrition_flag'].value_counts().plot(kind = 'bar')

# check churn_data structure
churn_data.info()

# Binning customer_age
age_bins = [0, 25, 35, 45, 55, 100]  # batas usia
age_labels = ['<25', '25-34', '35-44', '45-54', '55+']  # label kategori
churn_data['age_group'] = pd.cut(churn_data['customer_age'], bins=age_bins, labels=age_labels)

# Pastikan batas tertinggi sudah diatur lebih besar secara manual daripada nilai maksimum di months_on_book
tenure_bins = [0, 12, 24, 36, 48, 60, 999]  # Gunakan 999 sebagai batas tertinggi
tenure_labels = ['<12', '12-24', '25-36', '37-48', '49-60', '60+']

# Melakukan binning
churn_data['tenure_group'] = pd.cut(churn_data['months_on_book'], bins=tenure_bins, labels=tenure_labels)

# check data
churn_data.head()

# check numerical data distribution
churn_data.hist(bins=30, figsize = (10, 10));

plt.figure(figsize=(10, 6))
sns.histplot(data=churn_data, x='customer_age', hue='attrition_flag', kde=True, bins=15)
plt.title('Distribusi Usia Nasabah Berdasarkan Churn')
plt.xlabel('Usia')
plt.ylabel('Jumlah Nasabah')
plt.show()

"""Hubungan Usia dengan Churn:

* Nasabah yang churn (berwarna oranye) tampak relatif lebih sedikit dibandingkan nasabah yang bertahan (berwarna biru) di setiap kelompok usia.
* Meski churn terjadi di hampir setiap rentang usia, ada indikasi bahwa churn lebih cenderung terjadi di usia 40-50 tahun.
* Tingkat churn terlihat lebih tinggi pada kelompok usia 40-50 tahun dibandingkan kelompok usia lainnya. Kelompok ini memiliki jumlah nasabah yang churn lebih signifikan, meskipun secara total jumlah nasabah pada usia ini juga lebih tinggi.
* Di kelompok usia yang lebih tua (sekitar 60 tahun ke atas), jumlah nasabah yang churn tampak lebih kecil, tetapi ini juga dapat disebabkan oleh lebih sedikitnya jumlah nasabah pada kelompok usia ini.
"""

churn_data['attrition_flag'] = churn_data['attrition_flag'].replace({'Existing Customer': 0, 'Attrited Customer': 1})

churn_data['education_level'] = churn_data['education_level'].astype('category').cat.codes
churn_data['income_category'] = churn_data['income_category'].astype('category').cat.codes

# Step 2: Hitung korelasi
correlation_matrix = churn_data[['education_level', 'income_category', 'attrition_flag']].corr()

# Step 3: Plot korelasi menggunakan heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', fmt=".2f")
plt.title("Correlation Matrix: Education Level, Income Category, and Churn")
plt.show()

"""* Korelasi antara Tingkat Pendidikan (education_level) dan Kategori Pendapatan (income_category):
Nilai korelasi antara education_level dan income_category adalah -0.01, yang menunjukkan korelasi yang sangat lemah dan hampir tidak ada hubungan antara tingkat pendidikan dan kategori pendapatan. Ini berarti perubahan dalam tingkat pendidikan tidak terlalu berhubungan dengan kategori pendapatan.

* Korelasi antara Tingkat Pendidikan (education_level) dan Churn (attrition_flag):
Nilai korelasi antara education_level dan attrition_flag adalah 0.01, yang menunjukkan hubungan yang sangat lemah dan hampir tidak ada korelasi antara tingkat pendidikan dan kemungkinan churn. Artinya, tingkat pendidikan tidak memiliki pengaruh signifikan terhadap churn.

* Korelasi antara Kategori Pendapatan (income_category) dan Churn (attrition_flag):
Nilai korelasi antara income_category dan attrition_flag adalah 0.02, juga menunjukkan korelasi yang sangat lemah. Hal ini menunjukkan bahwa kategori pendapatan tidak memiliki pengaruh yang signifikan terhadap churn.
"""

# Crosstab antara gender dan churn
gender_churn = pd.crosstab(churn_data['gender'], churn_data['attrition_flag'])
print(gender_churn)

# Visualisasi
sns.countplot(data=churn_data, x='gender', hue='attrition_flag')
plt.title('Distribusi Gender terhadap Churn')
plt.show()

"""Interpretasi:

* Secara total, jumlah nasabah wanita yang churn (930) lebih tinggi dibandingkan pria (697), meskipun jumlah total nasabah wanita juga lebih tinggi.
"""

sns.boxplot(data=churn_data, x='attrition_flag', y='months_on_book')
plt.title('Hubungan antara Lama Menjadi Nasabah dengan Churn')
plt.xlabel('Status Churn')
plt.ylabel('Months on Book')
plt.show()

"""* Kedua kelompok (nasabah yang churn dan tidak churn) memiliki distribusi lama menjadi nasabah yang mirip, dengan nilai median yang hampir sama.
* Rentang interkuartil (IQR), yang menunjukkan 50% data di tengah, serupa untuk kedua kelompok. Ini menunjukkan bahwa tidak ada perbedaan signifikan dalam rata-rata lama waktu antara nasabah yang churn dan yang tidak churn.
* Terdapat outlier di kedua kelompok, dengan beberapa nasabah yang memiliki durasi sangat pendek atau sangat panjang sebagai nasabah.

**Kesimpulan** : Tidak terlihat adanya korelasi kuat antara lamanya nasabah menjadi pelanggan bank dan status churn. Mungkin ada faktor lain yang lebih berpengaruh dalam memprediksi churn pada dataset ini.
"""

sns.boxplot(data=churn_data, x='attrition_flag', y='contacts_count_12_mon')
plt.title('Frekuensi Interaksi dalam 1 Tahun terhadap Churn')
plt.xlabel('Status Churn')
plt.ylabel('Jumlah Interaksi (12 Bulan)')
plt.show()

"""* Median Interaksi: Pelanggan yang churn (status 1) memiliki median frekuensi interaksi yang lebih tinggi dibandingkan dengan yang tidak churn (status 0). Ini menunjukkan bahwa rata-rata, pelanggan yang sering berinteraksi cenderung lebih mungkin untuk churn.

* Rentang Interaksi: Rentang interaksi untuk pelanggan yang churn juga lebih lebar dibandingkan dengan yang tidak churn, menunjukkan variasi yang lebih besar dalam frekuensi interaksi.
* Secara keseluruhan, frekuensi interaksi tampaknya berkorelasi positif dengan kemungkinan churn. Semakin sering pelanggan berinteraksi dengan bank dalam satu tahun terakhir, tampaknya semakin besar kemungkinan mereka untuk churn.

# Feature Engineering
"""

# Encoding variabel kategorikal
churn_data = pd.get_dummies(churn_data, columns=['education_level', 'marital_status', 'income_category', 'card_category', 'gender'], drop_first=True)

# Feature Interactions
churn_data['relationship_month_product'] = churn_data['total_relationship_count'] * churn_data['months_on_book']
churn_data['trans_amt_utilization_ratio'] = churn_data['total_trans_amt'] * churn_data['avg_utilization_ratio']

#Aggregate Ratios and Differences
churn_data['credit_usage_ratio'] = churn_data['total_revolving_bal'] / churn_data['credit_limit']
churn_data['avg_spend_per_transaction'] = churn_data['total_trans_amt'] / churn_data['total_trans_ct']

# Trend-based Features
churn_data['activity_increase'] = np.where(churn_data['total_amt_chng_q4_q1'] > 1, 1, 0)  # Binary feature: 1 if transactions increased, else 0
churn_data['transaction_count_increase'] = np.where(churn_data['total_ct_chng_q4_q1'] > 1, 1, 0)  # Binary feature for transaction count increase

# Normalisasi/Konversi ke Skala Logaritmik
churn_data['log_credit_limit'] = np.log1p(churn_data['credit_limit'])
churn_data['log_total_trans_amt'] = np.log1p(churn_data['total_trans_amt'])
churn_data['log_total_revolving_bal'] = np.log1p(churn_data['total_revolving_bal'])

# Days Since Last Activity
churn_data['days_since_last_activity'] = (12 - churn_data['months_inactive_12_mon']) * 30

# Engagement Score
churn_data['engagement_score'] = churn_data['contacts_count_12_mon'] + churn_data['total_trans_ct'] + churn_data['total_relationship_count']

# check the first 5 rows
churn_data.head()

# drop certain columns
churn_data = churn_data.drop(['customer_age', 'months_on_book', 'total_trans_amt', 'credit_limit', 'total_trans_ct', 'total_revolving_bal','total_amt_chng_q4_q1','total_ct_chng_q4_q1', 'months_inactive_12_mon','contacts_count_12_mon','total_relationship_count' ], axis = 1)

# check the first 5 rows
churn_data.head()

# check the first 5 data after drop certain columns
churn_data.info()

# Mengonversi kolom kategori menjadi numerik (jika belum dilakukan encoding)
churn_data['age_group'] = churn_data['age_group'].cat.codes
churn_data['tenure_group'] = churn_data['tenure_group'].cat.codes

# check the first 5 data after drop certain columns
churn_data.head()

"""# Train-Test Split"""

# cretate predictor variables as X
X = churn_data.drop(['attrition_flag'], axis = 1)
# create target data as y
y = churn_data['attrition_flag']

# split data to train and test data
# where 30% of churn_data are test data
# stratify data based on y varibales
# and freeze RNG with random_state = 1000
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.3,
    stratify = y,
    random_state=1000
)

# check train data dimension
X_train.shape

# check test data dimension
X_test.shape

# check train target counts
y_train.value_counts()

# check test target counts
y_test.value_counts()

"""# Exploratory Data Analysis"""

# create correlation matrix
X_train.corr().style.background_gradient(cmap='coolwarm')

# drop certain column on train data
column_to_drop = ['avg_utilization_ratio', 'log_total_trans_amt', 'log_credit_limit', 'trans_amt_utilization_ratio']
X_train = X_train.drop(column_to_drop, axis = 1)

# drop certain column in test data
X_test = X_test.drop(column_to_drop, axis = 1)

# pull numerical column
var_name = X_train.select_dtypes(include = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns

var_name

# # create subplot 4 x 2
fig, axes = plt.subplots(4,2, sharex=False, sharey = False, figsize=(10,15), constrained_layout = True)

fig.suptitle('Numerical Predictors vs Target Variable')

col_index = 0

for row in range(4):
  for col in range(2):
    sns.boxplot(ax = axes[row,col], x=y_train, y = X_train[var_name[col_index]])
    axes[row,col].set_title(var_name[col_index])
    # axes[row,col].xticks(color='w')
    col_index += 1

"""# Modeling

**Random Forest Model**
```python
sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)
```
For further explanation, check: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
"""

# define random forest model
random_forest_clf = RandomForestClassifier(
    random_state = 1000,
    n_estimators=100
)
# fit model to training data
random_forest_clf.fit(X_train, y_train)

"""**XGBoost Classifier**
```python
xgboost.XGBClassifier(*, objective='binary:logistic', use_label_encoder=True, **kwargs)
```
For further explanation, check: https://xgboost.readthedocs.io/en/stable/python/python_api.html
"""

# fit model no training data
xgb_clf = XGBClassifier(
    random_state = 1000
)

xgb_clf.fit(X_train, y_train)

xgb_clf

"""# Model Evaluation"""

# random forest prediction
rf_pred = random_forest_clf.predict(X_test)
rf_pred_proba = random_forest_clf.predict_proba(X_test)

# XGBoost prediction
xgb_pred = xgb_clf.predict(X_test)
xgb_pred_proba = xgb_clf.predict_proba(X_test)

"""**Classification Report**"""

# random forest prediction result
pd.DataFrame(metrics.classification_report(y_test, rf_pred, target_names=['0','1'], output_dict=True))

# XGBoost prediction result
pd.DataFrame(metrics.classification_report(y_test, xgb_pred, target_names=['0','1'], output_dict=True))

"""**Confusion Matrix**"""

# random forest result
skplt.metrics.plot_confusion_matrix(y_test, rf_pred);

# xgboost result
skplt.metrics.plot_confusion_matrix(y_test, xgb_pred);

"""**ROC-AUC**"""

# random forest result
skplt.metrics.plot_roc_curve(y_test, rf_pred_proba);

# xgboost result
skplt.metrics.plot_roc_curve(y_test, xgb_pred_proba);

"""# Model Agnostic Methods"""

# Inititate Explainer for all models

## initiate explainer for Random Forest model
churn_rf_exp = dx.Explainer(random_forest_clf, X_train, y_train, label = "RF Interpretation")

## initiate explainer for XGBoost model
churn_xgb_exp = dx.Explainer(xgb_clf, X_train, y_train, label = "XGBoost Interpretation")

"""## Feature Importance"""

# visualizr permutation feature importance for Random Forest Model
churn_rf_exp.model_parts().plot()

# visualize permutation feature importance for XGBoost model
churn_xgb_exp.model_parts().plot()

"""## Partial Dependence Plot"""

# create partial dependence plot of Random Forest model
churn_rf_exp.model_profile().plot()

# create partial dependence plot of XGBoost model
churn_xgb_exp.model_profile().plot()

"""## Shapley Value and Shapley Additive Explanations

### Dataset Level Explanation

**Random Forest Model**
"""

# create sample used for developing shapley
data_for_shapley = X_train.sample(n = 1000, random_state = 1000)

# create SHAP Tree Explainer for random forest model with first 1000 rows of train data
rf_shap_values = shap.TreeExplainer(random_forest_clf).shap_values(data_for_shapley)

# create SHAP summary plot with next 1000 rows of train data
shap.summary_plot(rf_shap_values, X_train[1000:2000], plot_type='bar')

"""**XGBoost Model**"""

# create SHAP Tree Explainer for XGBoost model with| first 1000 rows of train data
xgb_shap_values = shap.TreeExplainer(xgb_clf).shap_values(X_train[:1000])

# create SHAP summary plot with next 1000 rows of train data
shap.summary_plot(xgb_shap_values, X_train[1000:2000], plot_type='bar')

# create SHAP summary plot to visualize impact distribution of next 1000 rows of train data
shap.summary_plot(xgb_shap_values, X_train[1000:2000])

"""### Instance Level"""

# initiate javascript module
shap.initjs()

# check data for row 1
X_train.iloc[1,:]

# check target for row 1
y_train[1]

"""**XGBoost Model**"""

# create SHAP Tree Explainer for XGBoost model with all rows of train data
xgb_explainer = shap.TreeExplainer(xgb_clf)
# create shap values from xgb_explainer
xgb_shap_values = xgb_explainer.shap_values(X_train)

# initiate javascript module
shap.initjs()

# explain prediction for the first row of X_train
shap.force_plot(xgb_explainer.expected_value, xgb_shap_values[1,:], X_train.iloc[1,:])

# initiate javascript module
shap.initjs()

# explain prediction for the first row of X_train
shap.force_plot(xgb_explainer.expected_value, xgb_shap_values[:1000,:], X_train.iloc[:1000,:])

"""## Local Interpretable Model-Agnostic Explanation"""

# define Random Forest explainer with lime module
lime_explainer = LimeTabularExplainer(
    X_train.values,
    feature_names = X_train.columns.tolist(),
    class_names = ['Existing', 'Churn'],
    discretize_continuous = True,
    verbose = True
)

user_id_23976 = X_train.iloc[0]
user_id_23976

# explain Random Forest prediction for user_id: 23976
lime_explainer.explain_instance(user_id_23976, random_forest_clf.predict_proba).show_in_notebook(show_table=True)

# explain Random Forest prediction for user_id: 23976 (sometimes error)
lime_explainer.explain_instance(user_id_23976, xgb_clf.predict_proba).show_in_notebook(show_table=True)

"""# Business Evaluation Metrics

## Gaim Cumulative Curve
"""

skplt.metrics.plot_cumulative_gain(y_test, xgb_pred_proba)

"""## Lift Curve"""

skplt.metrics.plot_lift_curve(y_test, xgb_pred_proba)

"""## Profit Curve"""

def standard_confusion_matrix(y_true, y_pred):
    '''
    Reformat confusion matrix output from sklearn for plotting profit curve.
    '''
    [[tn, fp], [fn, tp]] = metrics.confusion_matrix(y_true, y_pred)
    return np.array([[tp, fp], [fn, tn]])

def plot_profit_curve(model_object, costbenefit_mat, y_proba, y_test):
    '''
    Plot profit curve.

    INPUTS:
    - model object
    - cost benefit matrix in the same format as the confusion matrix above
    - predicted probabilities
    - actual labels
    '''

    # Profit curve data
    profits = [] # one profit value for each T (threshold)
    thresholds = sorted(y_proba, reverse=True)

    # For each threshold, calculate profit - starting with largest threshold
    for T in thresholds:
        y_pred = (y_proba > T).astype(int)
        confusion_mat = metrics.confusion_matrix(y_test, y_pred)
        # Calculate total profit for this threshold
        profit = sum(sum(confusion_mat * costbenefit_mat)) / len(y_test)
        profits.append(profit)

    print(profits)
    # Profit curve plot
    model_name = model_object.__class__.__name__
    max_profit = max(profits)
    plt.plot(np.linspace(0, 1, len(y_test)), profits, label = '{}, max profit ${:.2f}'.format(model_name, max_profit))

# create cost benefit matrix
costbenefit_mat = np.array([[0, -10],
                            [0, 20 - 10]])

# check cost benefit matrix
costbenefit_mat

# check model confusion matrix
skplt.metrics.plot_confusion_matrix(y_test, xgb_pred);

plot_profit_curve(xgb_clf, costbenefit_mat, xgb_pred_proba[:,1], y_test)
plt.xlabel('Percentage of test instances (decreasing by score)')
plt.ylabel('Profit')
plt.title('Profit Curves')
plt.legend(loc='lower left')
plt.show()

